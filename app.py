import streamlit as st
import pandas as pd
import re
import io
import json
from datetime import datetime

st.set_page_config(page_title="TxnGuard ‚Äì Duplicate Payment Detector", layout="wide", page_icon="üõ°Ô∏è")

st.markdown("""
<style>
.main-header { font-size: 2.2rem; font-weight: 800; color: #1e3a5f; letter-spacing: -0.5px; }
.sub-header  { color: #555; font-size: 1rem; margin-bottom: 0.4rem; }
.new-badge   { background:#dcfce7; color:#166534; padding:2px 8px; border-radius:10px;
               font-size:0.75rem; font-weight:700; margin-left:8px; }
.old-badge   { background:#f1f5f9; color:#64748b; padding:2px 8px; border-radius:10px;
               font-size:0.75rem; font-weight:600; margin-left:8px; }
</style>
""", unsafe_allow_html=True)

_hc1, _hc2 = st.columns([3, 1])
with _hc1:
    st.markdown('<div class="main-header">üõ°Ô∏è TxnGuard</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">Duplicate Payment Detector ‚Äî flags CR transactions where the same sender appears more than once across UPI, NEFT, RTGS & IMPS.</div>', unsafe_allow_html=True)
with _hc2:
    st.markdown("""
    <div style="text-align:right;padding-top:0.5rem;">
        <div style="font-size:0.72rem;color:#999;">Built by</div>
        <div style="font-weight:800;color:#1e3a5f;font-size:1.05rem;">Nishanth Fiona</div>
        <div style="font-size:0.8rem;color:#2563eb;font-weight:600;">Data Analyst</div>
    </div>
    """, unsafe_allow_html=True)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ACCOUNT ID EXTRACTION (unchanged logic)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
def extract_account_id(description: str) -> tuple:
    if not isinstance(description, str) or not description.strip():
        return None, "UNKNOWN"
    desc = description.strip()

    if re.search(r'\bUPI\b', desc, re.IGNORECASE):
        matches = re.findall(r'([\w.\-]{3,})@([\w]*)', desc)
        for prefix, suffix in matches:
            if len(prefix) > 30: continue
            if re.match(r'^[A-Z]{2,6}$', prefix): continue
            return f"upi:{prefix.lower()}", "UPI"
        return None, "UPI"

    if re.search(r'\b(NEFT|RTGS|INFT)\b', desc, re.IGNORECASE):
        txn_type = "RTGS" if re.search(r'\bRTGS\b', desc, re.IGNORECASE) else "NEFT"
        match = re.search(r'(\d{9,18})-([A-Z]{4}[0-9A-Z]{7})', desc)
        if match:
            return f"acct:{match.group(1)}", txn_type
        fallback = re.search(r'\b(\d{11,18})\b', desc)
        if fallback:
            return f"acct:{fallback.group(1)}", txn_type
        return None, txn_type

    if re.search(r'\b(IMPS|MMT)\b', desc, re.IGNORECASE):
        known_banks = re.compile(
            r'^(federal|hdfc|sbi|icici|axis|kotak|south indian|canara|pnb|bob|'
            r'idfc|yes|union|indian|karnataka|karur|city union|tamilnad|dcb|rbl|'
            r'indusind|bandhan|au small|ujjivan|equitas|jana|central|uco|'
            r'syndicate|corporation|allahabad|dena|vijaya|oriental|'
            r'state bank|bank of|standard chartered|citi|deutsche|hsbc|'
            r'baroda|punjab|federal bank|south indian ba)', re.IGNORECASE)
        parts = [p.strip() for p in desc.split('/')]
        if len(parts) >= 2:
            candidate = parts[-2].strip()
            skip_kw = re.compile(r'^(MMT|IMPS|NEFT|RTGS|UPI|\d+)$', re.IGNORECASE)
            if (candidate and not skip_kw.match(candidate)
                    and not known_banks.match(candidate)
                    and not candidate.isdigit() and len(candidate) > 2):
                return f"imps_name:{candidate.upper()}", "IMPS"
        return None, "IMPS"

    return None, "OTHER"

def clean_id(acct_id: str) -> str:
    return acct_id.replace("upi:", "").replace("acct:", "").replace("imps_name:", "‚ö†Ô∏è ")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FILE UPLOAD SECTION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
st.markdown("---")
col_txn, col_dec = st.columns([2, 1])

with col_txn:
    st.markdown("#### üìÇ Step 1 ‚Äî Upload Transaction File")
    uploaded_file = st.file_uploader("Excel or CSV file", type=["xlsx", "xls", "csv"], key="txn_file")

with col_dec:
    st.markdown("#### üíæ Step 2 ‚Äî Upload Previous Decisions *(optional)*")
    st.caption("Upload the decisions file saved from a previous session to skip re-reviewing old groups.")
    decisions_file = st.file_uploader("decisions_*.json file", type=["json"], key="dec_file")

if not uploaded_file:
    st.info("Upload your transaction file above to begin.")
    st.markdown("""
    **Workflow for daily use:**
    1. Upload your growing transaction file (all history)
    2. Upload your saved decisions file from yesterday *(optional but saves time)*
    3. Tool shows only **new unreviewed** duplicate groups at the top
    4. Review, mark decisions, then **save decisions file** for tomorrow

    | Type | ID used | Reliability |
    |---|---|---|
    | UPI | VPA username before `@` | ‚úÖ Reliable |
    | NEFT/RTGS | Account number before IFSC | ‚úÖ Reliable |
    | IMPS/MMT | Sender name (truncated) | ‚ö†Ô∏è Human review |
    """)
    st.stop()

# ‚îÄ‚îÄ Load transaction file ‚îÄ‚îÄ
try:
    df_raw = (pd.read_csv(uploaded_file) if uploaded_file.name.endswith(".csv")
              else pd.read_excel(uploaded_file))
except Exception as e:
    st.error(f"Could not read file: {e}")
    st.stop()

st.success(f"‚úÖ Loaded **{len(df_raw):,}** rows from `{uploaded_file.name}`")

# ‚îÄ‚îÄ Load previous decisions ‚îÄ‚îÄ
saved_decisions = {}   # txn_id -> {"decision": ..., "sender_id": ..., "saved_at": ...}
if decisions_file:
    try:
        saved_decisions = json.load(decisions_file)
        st.success(f"üíæ Loaded **{len(saved_decisions):,}** previously saved decisions.")
    except Exception as e:
        st.warning(f"Could not read decisions file: {e}")

# ‚îÄ‚îÄ Column selectors ‚îÄ‚îÄ
all_cols = list(df_raw.columns)

def best_match(candidates):
    for c in candidates:
        for col in all_cols:
            if c.lower() in col.lower():
                return col
    return all_cols[0]

with st.expander("‚öôÔ∏è Column Settings", expanded=False):
    c1, c2, c3 = st.columns(3)
    desc_col = c1.selectbox("Description column", all_cols,
                             index=all_cols.index(best_match(["description","desc","narration","particulars"])))
    crdr_options = ["(auto-detect CR)"] + all_cols
    crdr_col = c2.selectbox("CR/DR column", crdr_options,
                             index=crdr_options.index(best_match(["cr/dr","crdr","type","dr cr"]))
                             if best_match(["cr/dr","crdr","type","dr cr"]) in crdr_options else 0)
    txnid_options = ["(none)"] + all_cols
    txnid_col = c3.selectbox("Transaction ID column", txnid_options,
                              index=txnid_options.index(best_match(["transaction id","txn id","txnid","trans id"]))
                              if best_match(["transaction id","txn id","txnid","trans id"]) in txnid_options else 0)

# ‚îÄ‚îÄ Filter CR only ‚îÄ‚îÄ
df = df_raw.copy()
if crdr_col != "(auto-detect CR)":
    cr_mask = df[crdr_col].astype(str).str.strip().str.upper().isin(["CR", "CREDIT", "C"])
    df_cr   = df[cr_mask].copy()
else:
    df_cr = df.copy()

# ‚îÄ‚îÄ Extract IDs ‚îÄ‚îÄ
extracted = df_cr[desc_col].apply(extract_account_id)
df_cr = df_cr.copy()
df_cr["__account_id"] = extracted.apply(lambda x: x[0])
df_cr["__txn_type"]   = extracted.apply(lambda x: x[1])

# ‚îÄ‚îÄ Build transaction ID per row (for decision tracking) ‚îÄ‚îÄ
if txnid_col != "(none)" and txnid_col in df_cr.columns:
    df_cr["__txn_key"] = df_cr[txnid_col].astype(str).str.strip()
else:
    # Fallback: use row index as stable key
    df_cr["__txn_key"] = df_cr.index.astype(str)

df_identified   = df_cr[df_cr["__account_id"].notna()].copy()
df_unidentified = df_cr[df_cr["__account_id"].isna()].copy()

acct_counts  = df_identified["__account_id"].value_counts()
dup_accounts = acct_counts[acct_counts > 1]
df_identified["__is_dup"] = df_identified["__account_id"].isin(dup_accounts.index)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# DECISION STATE ‚Äî merge saved decisions with session state
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# saved_decisions = {txn_key: {decision, sender_id, saved_at}}
# session_state.txn_decisions = same structure (in-memory for this session)

if "txn_decisions" not in st.session_state:
    st.session_state.txn_decisions = {}

# Load saved decisions into session on first load of this file
file_sig = f"{uploaded_file.name}_{len(df_raw)}"
if st.session_state.get("__loaded_file") != file_sig:
    st.session_state.txn_decisions = dict(saved_decisions)
    st.session_state["__loaded_file"] = file_sig

def get_txn_decision(txn_key: str) -> str:
    entry = st.session_state.txn_decisions.get(txn_key)
    return entry["decision"] if entry else "‚è≥ Pending"

def set_txn_decision(txn_key: str, decision: str, sender_id: str):
    st.session_state.txn_decisions[txn_key] = {
        "decision": decision,
        "sender_id": sender_id,
        "saved_at": datetime.now().strftime("%Y-%m-%d %H:%M")
    }

def get_group_status(group_df) -> dict:
    """
    For a group of transactions, returns:
    - decisions: {txn_key: decision}
    - all_reviewed: bool (all txns have a non-Pending decision)
    - has_new: bool (at least one txn is Pending / not in saved_decisions)
    - consensus: the decision if all reviewed and same, else "Mixed"
    """
    decisions = {}
    for _, row in group_df.iterrows():
        key = row["__txn_key"]
        decisions[key] = get_txn_decision(key)
    
    values = list(decisions.values())
    all_reviewed = all(v != "‚è≥ Pending" for v in values)
    has_new      = any(v == "‚è≥ Pending" for v in values)
    
    unique_vals = set(v for v in values if v != "‚è≥ Pending")
    if all_reviewed and len(unique_vals) == 1:
        consensus = unique_vals.pop()
    elif all_reviewed:
        consensus = "Mixed"
    else:
        consensus = None

    return {"decisions": decisions, "all_reviewed": all_reviewed,
            "has_new": has_new, "consensus": consensus}

# ‚îÄ‚îÄ Metrics ‚îÄ‚îÄ
st.markdown("---")
m1, m2, m3, m4, m5 = st.columns(5)
m1.metric("Total Rows",         f"{len(df_raw):,}")
m2.metric("CR Transactions",    f"{len(df_cr):,}")
m3.metric("IDs Extracted",      f"{df_identified['__account_id'].nunique():,}")
m4.metric("Repeat Senders",     f"{len(dup_accounts):,}")
m5.metric("Flagged Txns",       f"{df_identified['__is_dup'].sum():,}")
st.markdown("---")

with st.expander("üìä Breakdown by transaction type", expanded=False):
    bd  = df_cr.groupby("__txn_type").size().reset_index(name="Total CR")
    ibd = df_identified.groupby("__txn_type").size().reset_index(name="ID Extracted")
    bd  = bd.merge(ibd, on="__txn_type", how="left").fillna(0)
    bd["ID Extracted"]    = bd["ID Extracted"].astype(int)
    bd["Skipped (no ID)"] = bd["Total CR"] - bd["ID Extracted"]
    bd.columns = ["Txn Type", "Total CR", "ID Extracted", "Skipped (no ID)"]
    st.dataframe(bd, use_container_width=True, hide_index=True)

display_cols = list(df_raw.columns)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TABS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
tab1, tab2, tab3, tab4 = st.tabs(["üö® Duplicate Accounts", "üìã All CR Transactions", "‚ùì Unidentified Rows", "üì• Export & Save"])

# ‚îÄ‚îÄ TAB 1: Duplicate Accounts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab1:
    if dup_accounts.empty:
        st.success("üéâ No duplicate sender account IDs found!")
    else:
        # Split groups into: needs review (has new txns) vs fully reviewed
        needs_review = []
        fully_reviewed = []

        for acct_id, count in dup_accounts.items():
            group_df = df_identified[df_identified["__account_id"] == acct_id].copy()
            status   = get_group_status(group_df)
            if status["has_new"]:
                needs_review.append((acct_id, count, group_df, status))
            else:
                fully_reviewed.append((acct_id, count, group_df, status))

        # ‚îÄ‚îÄ NEW / UNREVIEWED groups ‚îÄ‚îÄ
        st.markdown(f"### üÜï Needs Review ‚Äî {len(needs_review)} group(s)")
        if not needs_review:
            st.success("‚úÖ All groups have been reviewed!")
        else:
            st.caption("These groups contain at least one transaction not yet reviewed.")
            for acct_id, count, group_df, status in needs_review:
                txn_types  = group_df["__txn_type"].unique().tolist()
                new_count  = sum(1 for d in status["decisions"].values() if d == "‚è≥ Pending")
                old_count  = count - new_count

                # Check if this sender had ALL previous txns marked Legitimate
                # ‚Äî that means user may assume it's fine, but needs to look at new ones carefully
                prev_decisions = [d for k, d in status["decisions"].items() if d != "‚è≥ Pending"]
                prev_all_legit = prev_decisions and all(d == "‚úÖ Legitimate" for d in prev_decisions)

                label = (f"üîë {clean_id(acct_id)}  ({', '.join(txn_types)})  ¬∑  "
                         f"{count} transactions  "
                         f"[üÜï {new_count} new"
                         + (f"  ¬∑  {old_count} previously reviewed" if old_count else "")
                         + "]")

                with st.expander(label, expanded=True):

                    # ‚ö†Ô∏è Warning banner if sender was previously all-Legitimate
                    if prev_all_legit and new_count > 0:
                        st.warning(
                            f"‚ö†Ô∏è **Previously cleared sender** ‚Äî this sender's earlier transactions "
                            f"were all marked Legitimate, but **{new_count} new transaction(s) have appeared**. "
                            f"Please review each new transaction carefully before deciding.",
                            icon=None
                        )

                    # Show old (already reviewed) transactions first, greyed out
                    old_rows = [(idx, row) for idx, row in group_df[display_cols + ["__txn_key", "__txn_type"]].iterrows()
                                if get_txn_decision(row["__txn_key"]) != "‚è≥ Pending"]
                    new_rows = [(idx, row) for idx, row in group_df[display_cols + ["__txn_key", "__txn_type"]].iterrows()
                                if get_txn_decision(row["__txn_key"]) == "‚è≥ Pending"]

                    if old_rows:
                        st.markdown("<small style='color:#94a3b8;font-weight:600'>PREVIOUSLY REVIEWED</small>",
                                    unsafe_allow_html=True)
                    for _, row in old_rows:
                        txn_key    = row["__txn_key"]
                        cur_dec    = get_txn_decision(txn_key)
                        widget_key = f"txn_{txn_key}"
                        rc1, rc2   = st.columns([3, 1])
                        with rc1:
                            dec_icon = "‚úÖ" if "Legitimate" in cur_dec else "üö´"
                            st.markdown(
                                f"<small style='color:#94a3b8'>{dec_icon} <b>{txn_key}</b> ¬∑ {str(row[desc_col])[:90]}</small>",
                                unsafe_allow_html=True)
                        with rc2:
                            new_val = st.selectbox(
                                "Decision", options=["‚è≥ Pending", "‚úÖ Legitimate", "üö´ Flag as Duplicate"],
                                index=["‚è≥ Pending", "‚úÖ Legitimate", "üö´ Flag as Duplicate"].index(cur_dec),
                                key=widget_key, label_visibility="collapsed")
                            if new_val != cur_dec:
                                set_txn_decision(txn_key, new_val, acct_id)

                    if new_rows:
                        st.markdown("<small style='color:#dc2626;font-weight:700'>üÜï NEW ‚Äî REQUIRES YOUR REVIEW</small>",
                                    unsafe_allow_html=True)
                    for _, row in new_rows:
                        txn_key    = row["__txn_key"]
                        cur_dec    = get_txn_decision(txn_key)
                        widget_key = f"txn_{txn_key}"
                        rc1, rc2   = st.columns([3, 1])
                        with rc1:
                            st.markdown(
                                f"<small style='color:#1e293b;font-weight:600'>üÜï <b>{txn_key}</b> ¬∑ {str(row[desc_col])[:90]}</small>",
                                unsafe_allow_html=True)
                        with rc2:
                            new_val = st.selectbox(
                                "Decision", options=["‚è≥ Pending", "‚úÖ Legitimate", "üö´ Flag as Duplicate"],
                                index=["‚è≥ Pending", "‚úÖ Legitimate", "üö´ Flag as Duplicate"].index(cur_dec),
                                key=widget_key, label_visibility="collapsed")
                            if new_val != cur_dec:
                                set_txn_decision(txn_key, new_val, acct_id)

                    st.divider()
                    # No bulk quick-action buttons intentionally ‚Äî
                    # every new transaction must be reviewed individually.

        # ‚îÄ‚îÄ FULLY REVIEWED groups ‚îÄ‚îÄ
        st.markdown(f"---")
        st.markdown(f"### ‚úÖ Already Reviewed ‚Äî {len(fully_reviewed)} group(s)")
        if fully_reviewed:
            st.caption("All transactions in these groups have been reviewed. Expand to view or change.")
            for acct_id, count, group_df, status in fully_reviewed:
                txn_types = group_df["__txn_type"].unique().tolist()
                consensus = status["consensus"]
                icon      = "‚úÖ" if consensus == "‚úÖ Legitimate" else ("üö´" if consensus and "Duplicate" in consensus else "üîÄ")
                label     = f"{icon} {clean_id(acct_id)}  ({', '.join(txn_types)})  ¬∑  {count} transactions  [{consensus}]"

                with st.expander(label, expanded=False):
                    for _, row in group_df[display_cols + ["__txn_key"]].iterrows():
                        txn_key    = row["__txn_key"]
                        cur_dec    = get_txn_decision(txn_key)
                        widget_key = f"txn_{txn_key}"
                        rc1, rc2   = st.columns([3, 1])
                        with rc1:
                            desc_short = str(row[desc_col])[:90]
                            st.markdown(f"<small style='color:#555'><b>{txn_key}</b> ¬∑ {desc_short}</small>",
                                        unsafe_allow_html=True)
                        with rc2:
                            new_val = st.selectbox(
                                "Decision",
                                options=["‚è≥ Pending", "‚úÖ Legitimate", "üö´ Flag as Duplicate"],
                                index=["‚è≥ Pending", "‚úÖ Legitimate", "üö´ Flag as Duplicate"].index(cur_dec),
                                key=widget_key,
                                label_visibility="collapsed"
                            )
                            if new_val != cur_dec:
                                set_txn_decision(txn_key, new_val, acct_id)

        # Progress summary
        st.markdown("---")
        all_dup_txns = df_identified[df_identified["__is_dup"]]
        total_dup_txns = len(all_dup_txns)
        reviewed_count = sum(1 for _, row in all_dup_txns.iterrows()
                             if get_txn_decision(row["__txn_key"]) != "‚è≥ Pending")
        st.markdown(f"**Review progress: {reviewed_count} / {total_dup_txns} transactions reviewed**")
        st.progress(reviewed_count / total_dup_txns if total_dup_txns else 1.0)

# ‚îÄ‚îÄ TAB 2: All CR Transactions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab2:
    filter_opt = st.radio("Show:", ["All CR", "Flagged (repeat IDs)", "Clean", "No ID"], horizontal=True)
    view = df_cr.copy()
    view["Account ID"]  = view["__account_id"].fillna("‚Äî").apply(lambda x: clean_id(x) if isinstance(x, str) else "‚Äî")
    view["Txn Type"]    = view["__txn_type"]
    view["Repeat Flag"] = view["__account_id"].apply(
        lambda s: "üî¥ REPEAT" if s in dup_accounts.index else ("üü¢ UNIQUE" if pd.notna(s) else "‚ö™ NO ID"))
    view["Decision"]    = view["__txn_key"].apply(get_txn_decision)

    if filter_opt == "Flagged (repeat IDs)":
        view = view[view["__account_id"].isin(dup_accounts.index)]
    elif filter_opt == "Clean":
        view = view[view["__account_id"].notna() & ~view["__account_id"].isin(dup_accounts.index)]
    elif filter_opt == "No ID":
        view = view[view["__account_id"].isna()]

    show_cols = display_cols + ["Account ID", "Txn Type", "Repeat Flag", "Decision"]
    st.dataframe(view[show_cols].reset_index(drop=True), use_container_width=True, height=500)

# ‚îÄ‚îÄ TAB 3: Unidentified ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab3:
    st.markdown(f"### {len(df_unidentified):,} CR transactions ‚Äî no identifier extractable")
    st.caption("ATM, bank charges, reversals, or formats where no sender ID could be found.")
    if not df_unidentified.empty:
        show = df_unidentified[display_cols + ["__txn_type"]].rename(columns={"__txn_type": "Txn Type"})
        st.dataframe(show.reset_index(drop=True), use_container_width=True, height=450)

# ‚îÄ‚îÄ TAB 4: Export & Save ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with tab4:
    st.markdown("### üì• Export & üíæ Save Decisions")

    def to_excel_bytes(data: pd.DataFrame, sheet="Sheet1") -> bytes:
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="openpyxl") as writer:
            data.to_excel(writer, index=False, sheet_name=sheet)
            ws = writer.sheets[sheet]
            for col in ws.columns:
                w = max((len(str(c.value or "")) for c in col), default=10)
                ws.column_dimensions[col[0].column_letter].width = min(w + 4, 55)
        return buf.getvalue()

    # ‚îÄ‚îÄ SAVE DECISIONS FILE ‚îÄ‚îÄ
    st.markdown("#### üíæ Save Your Decisions (re-upload tomorrow to skip re-reviewing)")
    decisions_to_save = dict(st.session_state.txn_decisions)
    if decisions_to_save:
        dec_json = json.dumps(decisions_to_save, indent=2)
        st.download_button(
            "‚¨áÔ∏è Download decisions file (decisions.json)",
            data=dec_json,
            file_name=f"decisions_{datetime.now().strftime('%Y%m%d_%H%M')}.json",
            mime="application/json",
            help="Save this file and re-upload it next session ‚Äî all your decisions will be pre-filled automatically."
        )
        st.info(f"üì¶ {len(decisions_to_save)} transaction decisions saved in this file. "
                "Re-upload it alongside your transaction Excel next time to skip re-reviewing.")
    else:
        st.warning("No decisions made yet ‚Äî make some reviews in Tab 1 first.")

    st.markdown("---")

    # ‚îÄ‚îÄ EXCEL EXPORTS ‚îÄ‚îÄ
    export = df_cr.copy()
    export["Account ID"]  = export["__account_id"].fillna("").apply(lambda x: clean_id(x) if x else "")
    export["Txn Type"]    = export["__txn_type"]
    export["Repeat Flag"] = export["__account_id"].apply(
        lambda s: "REPEAT" if s in dup_accounts.index else ("UNIQUE" if pd.notna(s) and s else "NO_ID"))
    export["Decision"]    = export["__txn_key"].apply(get_txn_decision)
    export_cols = display_cols + ["Account ID", "Txn Type", "Repeat Flag", "Decision"]

    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("#### üî¥ Repeat Senders Only")
        dup_exp = export[export["__account_id"].isin(dup_accounts.index)][export_cols]
        st.info(f"{len(dup_exp)} transactions")
        st.download_button("‚¨áÔ∏è Download", data=to_excel_bytes(dup_exp, "RepeatAccounts"),
                           file_name=f"repeat_accounts_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                           use_container_width=True)
    with col2:
        st.markdown("#### üö´ Flagged as Duplicate")
        flag_exp = export[export["Decision"] == "üö´ Flag as Duplicate"][export_cols]
        st.info(f"{len(flag_exp)} transactions")
        if flag_exp.empty:
            st.warning("None flagged yet.")
        else:
            st.download_button("‚¨áÔ∏è Download", data=to_excel_bytes(flag_exp, "Flagged"),
                               file_name=f"flagged_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                               use_container_width=True)
    with col3:
        st.markdown("#### üìã Full CR Report")
        st.info(f"{len(export)} total CR transactions")
        st.download_button("‚¨áÔ∏è Download", data=to_excel_bytes(export[export_cols], "FullCR"),
                           file_name=f"full_cr_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                           use_container_width=True)

    # Summary
    st.markdown("---")
    st.markdown("#### üìä Repeat Sender Summary")
    if not dup_accounts.empty:
        rows = []
        for acct_id, cnt in dup_accounts.items():
            group_df = df_identified[df_identified["__account_id"] == acct_id]
            status   = get_group_status(group_df)
            types    = group_df["__txn_type"].unique().tolist()
            new_cnt  = sum(1 for d in status["decisions"].values() if d == "‚è≥ Pending")
            rows.append({
                "Account ID":  clean_id(acct_id),
                "ID Type":     "UPI VPA" if acct_id.startswith("upi:") else ("NEFT/RTGS Acct#" if acct_id.startswith("acct:") else "IMPS Name ‚ö†Ô∏è"),
                "Txn Type":    ", ".join(types),
                "Total Txns":  cnt,
                "New (unreviewed)": new_cnt,
                "Status":      status["consensus"] or "‚è≥ Partially reviewed",
            })
        summary = pd.DataFrame(rows).sort_values("New (unreviewed)", ascending=False).reset_index(drop=True)
        st.dataframe(summary, use_container_width=True, hide_index=True)
        st.download_button("‚¨áÔ∏è Download Summary",
                           data=to_excel_bytes(summary, "Summary"),
                           file_name=f"summary_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")